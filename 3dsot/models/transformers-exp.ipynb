{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fca2527c-1aec-4b6b-9159-e1707cb75c89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7824ca25-7646-4cf4-945b-659333db94dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, d_model=512, d_internal=64, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param d_model: The dimension of the inputs and outputs of the layer (note that the inputs and outputs\n",
    "        have to be the same size for the residual connection to work)\n",
    "        :param d_internal: The \"internal\" dimension used in the self-attention computation. Your keys and queries\n",
    "        should both be of this length.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_internal = d_internal\n",
    "        self.query = nn.Linear(d_model, d_internal)\n",
    "        self.key = nn.Linear(d_model, d_internal)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "#         self.linear = nn.Linear(d_model, d_model)\n",
    "#         self.relu = nn.ReLU()\n",
    "        \n",
    "#         self.linear2 = nn.Linear(d_model, d_internal)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "#         self.dropout2 = nn.Dropout(dropout)\n",
    "#         self.linear3 = nn.Linear(d_internal, d_model)\n",
    "#         self.dropout3 = nn.Dropout(dropout)\n",
    "#         self.layernorm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        \"\"\"\n",
    "        :param input_vecs: an input tensor of shape [seq len, d_model]\n",
    "        :return: a tuple of two elements:\n",
    "            - a tensor of shape [seq len, d_model] representing the log probabilities of each position in the input\n",
    "            - a tensor of shape [seq len, seq len], representing the attention map for this layer\n",
    "        \"\"\"\n",
    "        #n_pixels, batch, dim\n",
    "        q = self.query(query).permute(1, 0, 2) # batch, n_pixels, dim \n",
    "        k = self.key(key).permute(1, 0, 2) # batch, m_pixels, dim \n",
    "        v = self.value(value).permute(1, 0, 2) # batch, m_pixels, dim \n",
    "        q_k = torch.matmul(q, k.transpose(1,2)) # batch, n_pixels, m_pixels\n",
    "        q_k /= self.d_internal**0.5\n",
    "        probs = self.softmax(q_k)\n",
    "        probs /= (1e-9 + probs.sum(dim=1, keepdim=True))\n",
    "        aten_scores = torch.matmul(probs, v).permute(1,0,2)\n",
    "#         res_con = aten_scores + query\n",
    "#         aten_weights = self.linear(res_con)\n",
    "\n",
    "#         aten_weights = self.relu(aten_weights)\n",
    "#         aten_weights2 = self.linear2(aten_weights)\n",
    "#         aten_weights2 = self.relu2(aten_weights2)\n",
    "#         aten_weights2 = self.dropout2(aten_weights2)\n",
    "#         aten_weights2 = self.linear3(aten_weights2)\n",
    "#         aten_weights2 = self.dropout3(aten_weights2)\n",
    "#         aten_weights = aten_weights2 + aten_weights\n",
    "#         aten_weights = self.layernorm3(aten_weights)\n",
    "\n",
    "        return aten_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dc76918a-dacf-42a5-8a12-b5e4fce3b6c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = AttentionHead(512,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bb18c2f0-b8dc-4c01-a894-4305cbf9e517",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model=512, d_internal=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model, d_internal)\n",
    "        self.linear2 = nn.Linear(d_internal, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.instancenorm = nn.InstanceNorm1d(d_model)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        out = self.linear(input)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.dropout2(out)\n",
    "        out = out + input\n",
    "        \n",
    "        out = self.instancenorm(out.permute(1, 2, 0))\n",
    "        out = self.relu2(out.permute(2, 0, 1))\n",
    "        return out\n",
    "        \n",
    "\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, num_layers=1, d_model=512, d_internal=64):\n",
    "        \"\"\"\n",
    "        :param vocab_size: vocabulary size of the embedding layer\n",
    "        :param num_positions: max sequence length that will be fed to the model; should be 20\n",
    "        :param d_model: see TransformerLayer\n",
    "        :param d_internal: see TransformerLayer\n",
    "        :param num_classes: number of classes predicted at the output layer; should be 3\n",
    "        :param num_layers: number of TransformerLayers to use; can be whatever you want\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.attention_heads = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.attention_heads.append(AttentionHead(d_model, d_internal))\n",
    "                     \n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.feedforward = FeedForward(d_model, d_internal)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        \"\"\"\n",
    "\n",
    "        :param indices: list of input indices\n",
    "        :return: A tuple of the softmax log probabilities (should be a 20x3 matrix) and a list of the attention\n",
    "        maps you use in your layers (can be variable length, but each should be a 20x20 matrix)\n",
    "        \"\"\"\n",
    "        # inp = self.emb(indices)\n",
    "        aten_scores = None\n",
    "        first_layer = True\n",
    "        for attention_head in self.attention_heads:\n",
    "            if first_layer:\n",
    "                aten_scores = attention_head(q, k, v)\n",
    "                concat = aten_scores\n",
    "                first_layer = False\n",
    "            else:\n",
    "                aten_scores = attention_head(q,k,v)\n",
    "                concat = torch.cat((concat, aten_scores), -1)\n",
    "\n",
    "        out = self.linear(concat)\n",
    "        out = self.relu(out)\n",
    "        out = out + q\n",
    "        out = self.layernorm(out)\n",
    "        \n",
    "        out = self.feedforward(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "de81669d-62a6-42a1-8917-637960d43202",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int=256, num_positions: int=3, batched=False):\n",
    "        \"\"\"\n",
    "        :param d_model: dimensionality of the embedding layer to your model; since the position encodings are being\n",
    "        added to character encodings, these need to match (and will match the dimension of the subsequent Transformer\n",
    "        layer inputs/outputs)\n",
    "        :param num_positions: the number of positions that need to be encoded; the maximum sequence length this\n",
    "        module will see\n",
    "        :param batched: True if you are using batching, False otherwise\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(num_positions, d_model, kernel_size=1)\n",
    "        self.batchnorm = nn.BatchNorm1d(d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(d_model, d_model, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: If using batching, should be [batch size, seq len, embedding dim]. Otherwise, [seq len, embedding dim]\n",
    "        :return: a tensor of the same size with positional embeddings added in\n",
    "        \"\"\"\n",
    "        # B, n_pixels, 3\n",
    "        x = x.transpose(1,2).contiguous()\n",
    "        out = self.conv(x)\n",
    "        out = self.batchnorm(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "61251e37-4b8b-4bcc-a748-522e8e3758ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p = PositionalEncoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0bdb1cd4-c79f-4eff-8ce4-cef4804ffbd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0410,  0.0410,  0.0410,  ...,  0.0410,  0.0410,  0.0410],\n",
       "         [-0.0038, -0.0038, -0.0038,  ..., -0.0038, -0.0038, -0.0038],\n",
       "         [ 0.0067,  0.0067,  0.0067,  ...,  0.0067,  0.0067,  0.0067],\n",
       "         ...,\n",
       "         [-0.0259, -0.0259, -0.0259,  ..., -0.0259, -0.0259, -0.0259],\n",
       "         [-0.0021, -0.0021, -0.0021,  ..., -0.0021, -0.0021, -0.0021],\n",
       "         [ 0.0542,  0.0542,  0.0542,  ...,  0.0542,  0.0542,  0.0542]]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p(torch.ones(1,10,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd9719a-6e0c-4aee-a396-65cf2cf7d641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bc11f9-ed8f-41f7-b0af-ec5cbfb3c6fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceebd99-18d3-4733-91e9-a1c7f0d7ddf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "27555c2e-78b2-4ff6-a34c-48a25f8f3fe0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b = MultiheadAttention(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "29b11364-06a1-44a1-a395-9dd6691fc242",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 2 required positional arguments: 'k' and 'v'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/Open3DSOT/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 2 required positional arguments: 'k' and 'v'"
     ]
    }
   ],
   "source": [
    "b(torch.ones(1,10,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f39f0375-6e95-4144-a860-61c756e866fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2936,  1.0847,  0.2158,  ..., -0.0325,  0.5631,  0.3757],\n",
       "         [ 0.2936,  1.0847,  0.2158,  ..., -0.0325,  0.5631,  0.3757],\n",
       "         [ 0.2936,  1.0847,  0.2158,  ..., -0.0325,  0.5631,  0.3757]],\n",
       "\n",
       "        [[ 0.2936,  1.0847,  0.2158,  ..., -0.0325,  0.5631,  0.3757],\n",
       "         [ 0.2936,  1.0847,  0.2158,  ..., -0.0325,  0.5631,  0.3757],\n",
       "         [ 0.2936,  1.0847,  0.2158,  ..., -0.0325,  0.5631,  0.3757]]],\n",
       "       grad_fn=<PermuteBackward>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=torch.ones(2,3,512)\n",
    "a(z,z,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675bade5-e004-41fc-8b16-b8edeb198630",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.zeros((2,3,4))\n",
    "y = torch.zeros((2,3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7464c86-4ca4-4640-9e0c-1129b2670d45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y=y.transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc51d165-8a94-4c6b-8e36-29ace0cfbbfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df46dfb1-ddc0-473d-a291-1824fcec09ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "z=torch.matmul(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fd7ff6-b1ce-46d3-b057-460b6ef90121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7171ba-b7f4-43fa-9914-416e3d375565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ee97b358-80a3-41ca-89ea-ec61c4730510",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model=256, d_internal=128, n_heads=1, n_layers=1):\n",
    "        super().__init__()\n",
    "        encoder_layer = TransformerEncoderLayer(n_heads=n_heads, d_model=d_model,d_internal=d_internal)\n",
    "        self.encoder_layers = nn.ModuleList([encoder_layer for i in range(n_layers)])\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        intermediate = inp\n",
    "        for encoder in self.encoder_layers:\n",
    "            intermediate = encoder(intermediate)\n",
    "        return intermediate\n",
    "        \n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model=256, d_internal=128, n_heads=1):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiheadAttention(num_layers=n_heads, d_internal=d_internal,d_model=d_model)\n",
    "        \n",
    "        \n",
    "    def forward(self,inp):\n",
    "        return self.self_attention(inp, inp, inp)\n",
    "        \n",
    "        \n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model=256, d_internal=128, n_heads=1, n_layers=1):\n",
    "        super().__init__()\n",
    "        decoder_layer = TransformerDecoderLayer(n_heads=n_heads, d_model=d_model,d_internal=d_internal)\n",
    "        self.decoder_layers = nn.ModuleList([decoder_layer for i in range(n_layers)])\n",
    "        \n",
    "    def forward(self, inp_decoder, kv_encoder):\n",
    "        intermediate = inp_decoder\n",
    "        for decoder in self.decoder_layers:\n",
    "            intermediate = decoder(intermediate, kv_encoder)\n",
    "        return intermediate\n",
    "    \n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model=256, d_internal=128, n_heads=1):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiheadAttention(num_layers=n_heads, d_internal=d_internal,d_model=d_model)\n",
    "        self.cross_attention = MultiheadAttention(num_layers=n_heads, d_internal=d_internal,d_model=d_model)\n",
    "        \n",
    "        \n",
    "    def forward(self, inp, kv_encoder):\n",
    "        out = self.self_attention(inp, inp, inp)\n",
    "        res = self.cross_attention(out, kv_encoder, kv_encoder)\n",
    "        return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "39891094-5b90-456b-8d33-19494d51f0fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerFusion(nn.Module):\n",
    "    def __init__(self, num_layers_encoder = 1, num_layers_decoder=1, d_model=256, d_internal=128, n_heads=1):\n",
    "        super().__init__()\n",
    "        self.transformer_encoder = TransformerEncoder(d_model=d_model, d_internal=d_internal, n_heads=n_heads, n_layers=num_layers_encoder)\n",
    "        self.transformer_decoder = TransformerDecoder(d_model=d_model, d_internal=d_internal, n_heads=n_heads, n_layers=num_layers_decoder)\n",
    "        self.pos_emb_enc = PositionalEncoding(d_model=d_model, num_positions=3)\n",
    "        self.pos_emb_dec = PositionalEncoding(d_model=d_model, num_positions=3)\n",
    "        # self.feature_convs = (pt_utils.Seq(256)\n",
    "        #         .conv1d(256, bn=True)\n",
    "        #         .conv1d(256, activation=None))\n",
    "        \n",
    "    def forward(self, search_feature, search_xyz, template_feature, template_xyz):\n",
    "        search_feature = search_feature.permute(2,0,1) + self.pos_emb_dec(search_xyz).permute(2, 0, 1)\n",
    "        template_feature = template_feature.permute(2,0,1) + self.pos_emb_enc(template_xyz).permute(2,0,1)\n",
    "        kv_encoder = self.transformer_encoder(template_feature)\n",
    "        out = self.transformer_decoder(search_feature, kv_encoder)\n",
    "        out = out.permute(1,2,0)\n",
    "        # out = self.feature_convs(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "88036788-4665-40bd-abc0-0bb5b6f4e041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = TransformerFusion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "92d82954-4dcd-426a-945f-d078ea6a3bf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0554, 0.4846, 0.0000,  ..., 0.0000, 0.0000, 1.2612],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0653, 0.0000, 0.0000],\n",
       "         [0.4004, 2.3250, 0.0000,  ..., 0.0000, 0.2690, 0.0000],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.7656],\n",
       "         [0.1326, 0.0000, 1.5889,  ..., 0.0000, 1.5916, 0.0000],\n",
       "         [2.0896, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 1.2883, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.6922, 0.0000, 0.0000,  ..., 1.8916, 1.3286, 0.6990],\n",
       "         [0.0000, 1.4544, 0.9785,  ..., 0.3826, 0.0000, 1.0939],\n",
       "         ...,\n",
       "         [0.6329, 0.9107, 0.3306,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [2.5605, 0.0000, 0.7686,  ..., 0.0000, 0.0000, 0.3391],\n",
       "         [0.0000, 0.1047, 0.0000,  ..., 0.0000, 0.8144, 0.0000]]],\n",
       "       grad_fn=<PermuteBackward>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bcn\n",
    "t(torch.ones(2,256,10),torch.ones(2,10,3), torch.ones(2,256,10), torch.ones(2,10,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cae5f5-1fdd-4090-a443-b2d0c5b7c1e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b1e4cd-482e-4bcb-9f66-e88d8eb7751a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b03e523-690d-494f-9377-e015744024cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f2b3e-5468-4829-b6a4-7b16038152c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb2fb64-7e5b-4e58-b9da-97490f52ecb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20130c57-f237-4458-b2db-bbfcda630518",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02e26c8-219f-42d1-9ea0-530bf953e274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3454bc-cbb1-491a-9b81-5021f8c04d89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c876b751-9352-4cf0-9535-df736619ab27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "open3dsot",
   "name": "workbench-notebooks.m118",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m118"
  },
  "kernelspec": {
   "display_name": "Python (Open3DSOT) (Local)",
   "language": "python",
   "name": "open3dsot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
