{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fca2527c-1aec-4b6b-9159-e1707cb75c89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7824ca25-7646-4cf4-945b-659333db94dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, d_model=512, d_internal=64, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param d_model: The dimension of the inputs and outputs of the layer (note that the inputs and outputs\n",
    "        have to be the same size for the residual connection to work)\n",
    "        :param d_internal: The \"internal\" dimension used in the self-attention computation. Your keys and queries\n",
    "        should both be of this length.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_internal = d_internal\n",
    "        self.query = nn.Linear(d_model, d_internal)\n",
    "        self.key = nn.Linear(d_model, d_internal)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.linear2 = nn.Linear(d_model, d_internal)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.linear3 = nn.Linear(d_internal, d_model)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.layernorm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        \"\"\"\n",
    "        :param input_vecs: an input tensor of shape [seq len, d_model]\n",
    "        :return: a tuple of two elements:\n",
    "            - a tensor of shape [seq len, d_model] representing the log probabilities of each position in the input\n",
    "            - a tensor of shape [seq len, seq len], representing the attention map for this layer\n",
    "        \"\"\"\n",
    "\n",
    "        q = self.query(query).permute(1, 0, 2) # batch, n_pixels, dim \n",
    "        k = self.key(key).permute(1, 0, 2) # batch, m_pixels, dim \n",
    "        v = self.value(value).permute(1, 0, 2) # batch, m_pixels, dim \n",
    "        q_k = torch.matmul(q, k.transpose(1,2)) # batch, n_pixels, m_pixels\n",
    "        q_k /= self.d_internal**0.5\n",
    "        probs = self.softmax(q_k)\n",
    "        probs /= (1e-9 + probs.sum(dim=1, keepdim=True))\n",
    "        aten_scores = torch.matmul(probs, v).permute(1,0,2)\n",
    "        res_con = aten_scores + query\n",
    "        aten_weights = self.linear(res_con)\n",
    "\n",
    "        aten_weights = self.relu(aten_weights)\n",
    "        aten_weights2 = self.linear2(aten_weights)\n",
    "        aten_weights2 = self.relu2(aten_weights2)\n",
    "        aten_weights2 = self.dropout2(aten_weights2)\n",
    "        aten_weights2 = self.linear3(aten_weights2)\n",
    "        aten_weights2 = self.dropout3(aten_weights2)\n",
    "        aten_weights = aten_weights2 + aten_weights\n",
    "        aten_weights = self.layernorm3(aten_weights)\n",
    "\n",
    "        return aten_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc76918a-dacf-42a5-8a12-b5e4fce3b6c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = AttentionHead()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb18c2f0-b8dc-4c01-a894-4305cbf9e517",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, num_positions=3, num_layers=8, d_model=512, d_internal=64):\n",
    "        \"\"\"\n",
    "        :param vocab_size: vocabulary size of the embedding layer\n",
    "        :param num_positions: max sequence length that will be fed to the model; should be 20\n",
    "        :param d_model: see TransformerLayer\n",
    "        :param d_internal: see TransformerLayer\n",
    "        :param num_classes: number of classes predicted at the output layer; should be 3\n",
    "        :param num_layers: number of TransformerLayers to use; can be whatever you want\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.attention_heads = nn.Sequential(*[nn.Sequential(AttentionHead(d_model, d_internal))\n",
    "                     for i in range(num_layers)])\n",
    "        # self.linear = nn.Linear(d_model, 3)\n",
    "        # self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        \"\"\"\n",
    "\n",
    "        :param indices: list of input indices\n",
    "        :return: A tuple of the softmax log probabilities (should be a 20x3 matrix) and a list of the attention\n",
    "        maps you use in your layers (can be variable length, but each should be a 20x20 matrix)\n",
    "        \"\"\"\n",
    "        # inp = self.emb(indices)\n",
    "        input_vecs = self.positional_encoding(inp)\n",
    "        aten_probs = input_vecs\n",
    "        for attention_head in self.attention_heads:\n",
    "            aten_probs = attention_head(aten_probs, aten_probs, aten_probs)\n",
    "\n",
    "        # out = self.linear(aten_probs)\n",
    "        # out = self.softmax(out)\n",
    "\n",
    "        return aten_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de81669d-62a6-42a1-8917-637960d43202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61251e37-4b8b-4bcc-a748-522e8e3758ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdb1cd4-c79f-4eff-8ce4-cef4804ffbd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd9719a-6e0c-4aee-a396-65cf2cf7d641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bc11f9-ed8f-41f7-b0af-ec5cbfb3c6fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceebd99-18d3-4733-91e9-a1c7f0d7ddf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27555c2e-78b2-4ff6-a34c-48a25f8f3fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f39f0375-6e95-4144-a860-61c756e866fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8626, 0.5521, 0.0000,  ..., 0.0000, 0.0000, 0.1414],\n",
       "         [0.8626, 0.5521, 0.0000,  ..., 0.0000, 0.0000, 0.1414],\n",
       "         [0.8626, 0.5521, 0.0000,  ..., 0.0000, 0.0000, 0.1414]],\n",
       "\n",
       "        [[0.8626, 0.5521, 0.0000,  ..., 0.0000, 0.0000, 0.1414],\n",
       "         [0.8626, 0.5521, 0.0000,  ..., 0.0000, 0.0000, 0.1414],\n",
       "         [0.8626, 0.5521, 0.0000,  ..., 0.0000, 0.0000, 0.1414]]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(torch.ones(2,3,512),torch.ones(2,3,512),torch.ones(2,3,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "675bade5-e004-41fc-8b16-b8edeb198630",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.zeros((2,3,4))\n",
    "y = torch.zeros((2,3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7464c86-4ca4-4640-9e0c-1129b2670d45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y=y.transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc51d165-8a94-4c6b-8e36-29ace0cfbbfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df46dfb1-ddc0-473d-a291-1824fcec09ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "z=torch.matmul(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66fd7ff6-b1ce-46d3-b057-460b6ef90121",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7171ba-b7f4-43fa-9914-416e3d375565",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "open3dsot",
   "name": "workbench-notebooks.m118",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m118"
  },
  "kernelspec": {
   "display_name": "Python (Open3DSOT) (Local)",
   "language": "python",
   "name": "open3dsot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
